{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Requirement already satisfied: pyro-ppl in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (1.3.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (3.2.1)\nRequirement already satisfied: pyro-api>=0.1.1 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (0.1.2)\nRequirement already satisfied: numpy>=1.7 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (1.18.1)\nRequirement already satisfied: tqdm>=4.36 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (4.46.0)\nRequirement already satisfied: torch>=1.4.0 in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from pyro-ppl) (1.5.0)\nRequirement already satisfied: future in c:\\programdata\\anaconda3\\envs\\mbml\\lib\\site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n"
    }
   ],
   "source": [
    "!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.distributions import constraints\n",
    "import functools\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
    "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, AutoGuideList, AutoDelta\n",
    "from pyro.optim import ClippedAdam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the modifed data matrix, where the documents are of equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(32211, 1500)"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "import requests, io\n",
    "r = requests.get('https://github.com/MikkelGroenning/MBML_project/blob/master/data/processed/upsampled_data.npy?raw=true')\n",
    "\n",
    "data = np.load(io.BytesIO(r.content)).astype('int32')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of 32.211 speeches, each with a length of 1500 words. We therefore only look at a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'max'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-5303c3e3b25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdata_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sub\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# data[:n]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnum_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_sub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnum_topics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnum_docs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'int' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "data_sub = len(np.unique(data_sub)) # data[:n]\n",
    "num_words = data_sub.max() + 1\n",
    "num_topics = 25\n",
    "num_docs = n\n",
    "num_words_per_doc = data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these things defined we can now make a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data=None, batch_size=1):\n",
    "    \"\"\" Make a plate of size num_topics with name \"topics\" and define a variable \"topic_words\".\n",
    "          This represents the phi above. Use the equivalent of a uniform distribution for it  \"\"\"\n",
    "    with pyro.plate(\"topics\", num_topics):\n",
    "        topic_words = pyro.sample(\"topic_words\", dist.Dirichlet(torch.ones(num_words) / num_words))\n",
    "\n",
    "    \"\"\" Make two (nested) plates in here. One over documents and one over words\n",
    "          Documents, called \"documents\":\n",
    "          The plate over the documents should hold a variable \"doc_topics\" representing the theta above.\n",
    "            Use the equivalent of a uniform distribution for it.\n",
    "          \n",
    "          Words, called \"words\":\n",
    "          The plate over words, should have a topic assignment for each word (z_{i,j} above) which \n",
    "            should be enumerated.\n",
    "          The second variable should be the words themselves which should be drawn from the \"topic_words\"\n",
    "            using the assigned z_{i,j} and the observed data.\n",
    "\n",
    "     \"\"\"\n",
    "    with pyro.plate(\"documents\", num_docs) as ind:\n",
    "        if data is not None:\n",
    "            with pyro.util.ignore_jit_warnings():\n",
    "                assert data.shape == (num_words_per_doc, num_docs)\n",
    "            data = data[:, ind]\n",
    "        doc_topics = pyro.sample(\"doc_topics\", dist.Dirichlet(torch.ones(num_topics)/ num_topics))\n",
    "        with pyro.plate(\"words\", num_words_per_doc):\n",
    "            # The word_topics variable is marginalized out during inference,\n",
    "            # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
    "            # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
    "            # the guide.\n",
    "            word_topics = pyro.sample(\"word_topics\", dist.Categorical(doc_topics), infer={\"enumerate\": \"parallel\"})\n",
    "            data = pyro.sample(\"doc_words\", dist.Categorical(topic_words[word_topics]), obs=data)\n",
    "\n",
    "    return topic_words, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_torch = torch.tensor(data_sub.T).long()\n",
    "W_torch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use amortized inference of the local variables. This is acheived by using a multi-layer perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = np.arange(98,103)\n",
    "layer_sizes = torch.tensor(layer_sizes)\n",
    "print(layer_sizes.size())\n",
    "\n",
    "def make_predictor(num_words, layer_sizes):\n",
    "    layer_sizes = ([num_words] +\n",
    "                   [int(s) for s in torch.split(layer_sizes,1)] +\n",
    "                   [num_topics])\n",
    "    logging.info('Creating MLP with sizes {}'.format(layer_sizes))\n",
    "    layers = []\n",
    "    for in_size, out_size in zip(layer_sizes, layer_sizes[1:]):\n",
    "        layer = nn.Linear(in_size, out_size)\n",
    "        layer.weight.data.normal_(0, 0.001)\n",
    "        layer.bias.data.normal_(0, 0.001)\n",
    "        layers.append(layer)\n",
    "        layers.append(nn.Sigmoid())\n",
    "    layers.append(nn.Softmax(dim=-1))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parametrized_guide(predictor, data, batch_size=None):\n",
    "    # Use a conjugate guide for global variables.\n",
    "    topic_weights_posterior = pyro.param(\n",
    "            \"topic_weights_posterior\",\n",
    "            lambda: torch.ones(num_topics),\n",
    "            constraint=constraints.positive)\n",
    "    topic_words_posterior = pyro.param(\n",
    "            \"topic_words_posterior\",\n",
    "            lambda: torch.ones(num_topics, num_words),\n",
    "            constraint=constraints.greater_than(0.5))\n",
    "    with pyro.plate(\"topics\", num_topics):\n",
    "        pyro.sample(\"topic_weights\", dist.Gamma(topic_weights_posterior, 1.))\n",
    "        pyro.sample(\"topic_words\", dist.Dirichlet(topic_words_posterior))\n",
    "\n",
    "    # Use an amortized guide for local variables.\n",
    "    pyro.module(\"predictor\", predictor)\n",
    "    with pyro.plate(\"documents\", num_docs, batch_size) as ind:\n",
    "        data = data[:, ind]\n",
    "        # The neural network will operate on histograms rather than word\n",
    "        # index vectors, so we'll convert the raw data to a histogram.\n",
    "        counts = (torch.zeros(num_words, ind.size(0)).scatter_add(0, data, torch.ones(data.shape)))\n",
    "        doc_topics = predictor(counts.transpose(0, 1))\n",
    "        pyro.sample(\"doc_topics\", dist.Delta(doc_topics, event_dim=1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train using SVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_topics)\n",
    "print(num_words)\n",
    "print(num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6960600000 bytes. Buy new RAM!\n(no backtrace available)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-da4fbe190974>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# do gradient steps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0melbo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW_torch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#print('.', end='')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\pyro\\infer\\svi.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         \u001b[1;31m# get loss and compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\pyro\\infer\\traceenum_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[1;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtrainable_params\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melbo_particle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[0mloss_particle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0melbo_particle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                 \u001b[1;33m(\u001b[0m\u001b[0mloss_particle\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m         \"\"\"\n\u001b[1;32m--> 198\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\mbml\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 6960600000 bytes. Buy new RAM!\n(no backtrace available)"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "learning_rate = 0.05\n",
    "\n",
    "predictor = make_predictor(num_words, layer_sizes)\n",
    "guide = functools.partial(parametrized_guide, predictor)\n",
    "# Elbo = JitTraceEnum_ELBO if args.jit else TraceEnum_ELBO\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=3)\n",
    "optim = ClippedAdam({'lr': learning_rate})\n",
    "svi = SVI(model, guide, optim, loss=elbo)\n",
    "\n",
    "# Define the number of optimization steps\n",
    "n_steps = 5\n",
    "\n",
    "# do gradient steps\n",
    "for step in range(n_steps):\n",
    "    elbo = svi.step(W_torch, batch_size=2)\n",
    "    if step % 1 == 0:\n",
    "        #print('.', end='')\n",
    "        print(\"[%d] ELBO: %.1f\" % (step, elbo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmbmlcondafd08ce0341164d10a999be67eed30d97",
   "display_name": "Python 3.7.7 64-bit ('mbml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}