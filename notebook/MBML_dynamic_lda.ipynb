{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MBML_dynamic_lda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJFHkhntTAjY",
        "colab_type": "code",
        "outputId": "ff9de362-24da-4d41-fa61-598f26cab437",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "!pip install pyro-ppl"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/77/4db4946f6b5bf0601869c7b7594def42a7197729167484e1779fff5ca0d6/pyro_ppl-1.3.1-py3-none-any.whl (520kB)\n",
            "\u001b[K     |████████████████████████████████| 522kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.4)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.5.0+cu101)\n",
            "Collecting pyro-api>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fc/81/957ae78e6398460a7230b0eb9b8f1cb954c5e913e868e48d89324c68cec7/pyro_api-0.1.2-py3-none-any.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->pyro-ppl) (0.16.0)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McCzQ6nTSzQd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.distributions import constraints\n",
        "import functools\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "from pyro.infer import SVI, JitTraceEnum_ELBO, TraceEnum_ELBO\n",
        "from pyro.contrib.autoguide import AutoDiagonalNormal, AutoMultivariateNormal, AutoGuideList, AutoDelta\n",
        "from pyro.optim import ClippedAdam\n",
        "\n",
        "import time\n",
        "\n",
        "# fix MCMCndom generator seed (for reproducibility of results)\n",
        "np.random.seed(42)\n",
        "\n",
        "# matplotlib style options\n",
        "plt.style.use('ggplot')\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (16, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11NoQQxbTtUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def model(data=None, T, obs=None):\n",
        "    \"\"\" Define priors over beta1, beta2, tau, noises, sigma, z_prev1 and z_prev2 (keep the shapes in mind)\n",
        "    # Your code here\n",
        "    \"\"\"\n",
        "\n",
        "    psi = pyro.sample(name=\"psi\", fn=dist.HalfCauchy(scale=3.))\n",
        "    delta = pyro.sample(name=\"delta\", fn=dist.HalfCauchy(scale=3.))\n",
        "    sigma = pyro.sample(name=\"sigma\", fn=dist.HalfCauchy(scale=3.))\n",
        "\n",
        "    alpha_prev = pyro.sample(name=\"alpha_prev\", fn=dist.Normal(loc=torch.zeros(100), scale=delta))\n",
        "    beta_prev = pyro.sample(name=\"beta_prev\", fn=dist.Normal(loc=toruch.zeros(25), scale=sigma))\n",
        "    \n",
        "    ALPHA = [alpha_prev] \n",
        "    BETA =  [beta_prev]\n",
        "\n",
        "    for t in range(1, T):\n",
        "        alpha_t = pyro.sample(name=\"alpha_prev\", fn=dist.Normal(loc=alpha_prev, scale=delta))\n",
        "        beta_t = pyro.sample(name=\"beta_prev\", fn=dist.Normal(loc=beta_prev, scale=sigma))\n",
        "        \n",
        "        ALPHA.append(alpha_t)\n",
        "        BETA.append(beta_t)\n",
        "\n",
        "        alpha_prev = alpha_t\n",
        "        beta_prev = beta_t \n",
        "\n",
        "        \"\"\" Make a plate of size num_topics with name \"topics\" and define a variable \"topic_words\".\n",
        "          This represents the phi above. Use the equivalent of a uniform distribution for it  \"\"\"\n",
        "        # with pyro.plate(\"topics\", num_topics):\n",
        "                # topic_words\n",
        "                # theta = pyro.sample(\"theta\", dist.Normal(loc = alpha_t, scale = psi))\n",
        "\n",
        "            \"\"\" Make two (nested) plates  in here. One over documents and one over words\n",
        "                Documents, called \"documents\":\n",
        "                The plate over the documents should hold a variable \"doc_topics\" representing the theta above.\n",
        "                    Use the equivalent of a uniform distribution for it.\n",
        "                \n",
        "                Words, called \"words\":\n",
        "                The plate over words, should have a topic assignment for each word (z_{i,j} above) which \n",
        "                    should be enumerated.\n",
        "                The second variable should be the words themselves which should be drawn from the \"topic_words\"\n",
        "                    using the assigned z_{i,j} and the observed data.\n",
        "\n",
        "            \"\"\"\n",
        "        with pyro.plate(\"documents\", num_docs) as ind:\n",
        "            if data is not None:\n",
        "                with pyro.util.ignore_jit_warnings():\n",
        "                    assert data.shape == (num_words_per_doc, num_docs)\n",
        "                data = data[:, ind]\n",
        "            \n",
        "            theta = pyro.sample(\"theta\", dist.Normal(loc = alpha_t, scale = psi))\n",
        "\n",
        "            # doc_topics = pyro.sample(\"doc_topics\", dist.Categorical(torch.ones(num_topics)/ num_topics))\n",
        "            \n",
        "            with pyro.plate(\"words\", num_words_per_doc):\n",
        "                # The word_topics variable is marginalized out during inference,\n",
        "                # achieved by specifying infer={\"enumerate\": \"parallel\"} and using\n",
        "                # TraceEnum_ELBO for inference. Thus we can ignore this variable in\n",
        "                # the guide.\n",
        "\n",
        "\n",
        "                z = pyro.sample(\"z\", dist.Categorical(logits = theta))\n",
        "                w = pyro.sample(\"w\", dist.Categorical(logits = beta_t[z]]), obs=data)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8psn94Kxq7Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66b5cea3-ff56-417d-dc77-8e1cda32a596"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Normal(loc: torch.Size([100]), scale: torch.Size([100]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Kn9tIUTqw-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}